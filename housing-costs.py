# -*- coding: utf-8 -*-
"""Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CNFRuSup3NrkMg7TUf-snxCd1prZkjaM

# Final Project (Fall 2024)

**Date:** December 7th, 2024

**Team Members:**

* Brianna Huang
* Jacky Choi
* Cindy (Yongxue) Tao

# Background

As of May 2024, \$1.5 million is the average cost of a home in the Bay Area. Compared to the national average cost at around $420k, the Bay Area is known to be one of the most expensive housing markets in the past few decades. Certain rumors such as the dotcom bubble, good weather, or even just general demand for living spaces have been spreading leaving many people to believe these are the main causes of the ever growing prices. Today, we will deeply discover the main drivers of this expensive housing market and make predictions of future expensive housing markets with big data analytics.

# Setup and Libraries
"""

# Import Packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import geopandas as gpd
import folium
from folium.plugins import HeatMap
from folium import plugins
from branca.colormap import linear
from shapely.wkt import loads
from shapely.geometry import MultiPolygon, Polygon
import branca.colormap as cm
import requests
from requests.structures import CaseInsensitiveDict
from multiprocessing import Pool

#Stop us from getting annoying warnings that show up at the output
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

"""# Data Downloading

### User Authentication & Data Downloading Function Definition
"""

# Same method as the sample project
# download data set from server
# authenticate the user
from google.colab import auth
auth.authenticate_user()
from googleapiclient.discovery import build
drive_service = build('drive', 'v3')
import io
import os
from googleapiclient.http import MediaIoBaseDownload

def download_file(fn, file_id):
    request = drive_service.files().get_media(fileId=file_id)
    downloaded = io.BytesIO()
    downloader = MediaIoBaseDownload(downloaded, request)
    done = False
    while done is False:
        # _ is a placeholder for a progress object that we ignore.
        # (Our file is small, so we skip reporting progress.)
        _, done = downloader.next_chunk()
    downloaded.seek(0)
    folder = fn.split('/')
    if len(folder) > 1:
        os.makedirs(folder[0], exist_ok=True)
    with open(fn, 'wb') as f:
        f.write(downloaded.read())

"""### Download Datasets for the Project

Datasets that we decided to use are as following:


*   realtor-data.csv
*   cost-of-living-us.csv
*   cites-to-county.csv
*   salary-sf.csv
*   prop-21.csv
*   air-pollution.csv
*   us-city-avg-temp.csv
*   us-tech-companies.csv
*   ca-housing.csv
*   sf-precincts.csv
*   zoning.csv
*   bay-area.csv
*   cal_cities_lat_long.csv
"""

id_to_fn ={
'1exLzjvcGJZp6XAng8S0EbQQKyaOUJVmP': 'realtor-data.csv',
'1Z9Cy3Aj_i65ME8f0ihVl0zjl-frD8ERI': 'property-tax-ca.csv',
'1VGEy1kxOa3PF0Iej-MaLE_TdLes0AP1E': 'cost-of-living-us.csv',
'1UR1u44mE-aS-80sVhQLm8GPcoEQWDhP1': 'cites-to-county.csv',
'1m8njt-xQ8JtZVoAH8AXLAVgJs8NNSwf6': 'salary-sf.csv',
'1mswxPlKlVyXE6fBH4edceDDco5dx9Fkg': 'prop-21.csv',
'1YtscVKKMSIiF3xtPCQwIefhv3qzy9HOJ': 'air-pollution.csv',
'19qamnetMVt52l--HBy7dQWUwYX4T26EF': 'us-city-avg-temp.csv',
'1kdiBtllIfkuEU3KqaxVaEUAIo9Yg5RIc': 'us-tech-companies.csv',
'1IhKmR7Yo5IbE9RaGoMGsRrBOt4jhNieZ': 'ca-housing.csv',
'1Z13dlz_t9JRAJl8vIcwqc0nzDePUi_KM': 'sf-precincts.csv',
'1X4EcSaLYj-Nhe9hUYERbfpb-wZ1XQdzz': 'zoning.csv',
'1f4p6-aJma_wBI16rXOpIf8aCa5TkW-Zi': 'bay-area.csv',
'1hiopBzMSlO4sCi-XI3n4wsp4YkgYZq0f': 'cal_cities_lat_long.csv'
}

# download all files into the vm
for fid, fn in id_to_fn.items():
    print("Downloading %s from %s" % (fn, fid))
    download_file(fn, fid)

"""# Dataset Cleaning and Preparation

### Realtor Data Extraction & Cleaning

*   We extract data from realtor containing house data all around US
*   We further extracted the data that belongs to the San Francisco and California
"""

# Read datasets
realtor_df = pd.read_csv('realtor-data.csv')
realtor_df.head(3)

# Number of rows in the dateset
realtor_df.shape[0]

# Count num of unique cities
realtor_df['city'].nunique()

# Gather all the bay area zip code
bay_area_zipcodes = {
    'San Francisco': [94102, 94103, 94104, 94105, 94107, 94108, 94109, 94110, 94111, 94112, 94114, 94115, 94116, 94117, 94118, 94121, 94122, 94123, 94124, 94127, 94131, 94132, 94133, 94134],
    'Oakland': [94601, 94602, 94603, 94605, 94606, 94607, 94608, 94609, 94610, 94611, 94612, 94618, 94619, 94621],
    'San Jose': [95101, 95110, 95111, 95112, 95113, 95116, 95117, 95118, 95119, 95120, 95121, 95122, 95123, 95124, 95125, 95126, 95127, 95128, 95129, 95130, 95131, 95132, 95133, 95134, 95135, 95136, 95138, 95139, 95140],
    'Berkeley': [94701, 94702, 94703, 94704, 94705, 94706, 94707, 94708, 94709, 94710],
    'Fremont': [94536, 94537, 94538, 94539, 94555],
    'Union City': [94587],
    'Palo Alto': [94301, 94303, 94304, 94305, 94306],
    'Mountain View': [94035, 94039, 94040, 94041, 94043],
    'Santa Clara': [95050, 95051, 95052, 95053, 95054, 95055],
    'Hayward': [94540, 94541, 94542, 94543, 94544],
    'Dublin': [94568],
    'Pleasanton': [94566],
    'Livermore': [94550, 94551],
    'San Mateo': [94401, 94402, 94403, 94404, 94497],
    'Richmond': [94801, 94802, 94804, 94805, 94806, 94807, 94808],
    'Concord': [94518, 94519, 94520, 94521, 94522],
}

# Combine all ZIP codes into one list
combined_zipcodes = [zipcode for zipcodes in bay_area_zipcodes.values() for zipcode in zipcodes]

# Print the combined list
print(combined_zipcodes)

# Filter all the real estate in California
california_df = realtor_df[(realtor_df['state'] == 'California')]

# Filter all the real estate in the Bay Area
bay_area_df = realtor_df[(realtor_df['state'] == 'California') & (realtor_df['zip_code'].isin(combined_zipcodes))]
bay_area_df.head()

# Number of rows in the bay area dateset
bay_area_df.shape[0]

# Clean the data by dropping the na
california_df.dropna(subset=['zip_code', 'city'])
bay_area_df.dropna(subset=['zip_code', 'city'])
california_df.reset_index(drop=True, inplace=True)
bay_area_df.reset_index(drop=True, inplace=True)
california_df.head()

# Number of rows in the California dataset
california_df.shape[0]

"""### Salaries San Francisco Extraction & Cleaning

*   We explore the San Francisco salary dataset
"""

# Read in the San Francisco dataset
salaries_sf_df = pd.read_csv('salary-sf.csv')
salaries_sf_df.head()

# Number of rows in the San Francisco dataset
salaries_sf_df.shape[0]

# Clean the dataset by dropping the na and removing the duplicates
salaries_sf_df.fillna({'BasePay': 0, 'TotalPay': 0}, inplace = True)
salaries_sf_df.drop_duplicates(inplace=True)
salaries_sf_df.head()

"""### Cities to County Data Extraction & Cleaning

*   We added a cities to country dataset to help with merging some of the dataset that only had cities or only had county information
"""

# Read and clean the cities to county dataset
cities_to_county_df = pd.read_csv('cites-to-county.csv')
cities_to_county_df = cities_to_county_df[['city','county_name']]
cities_to_county_df = cities_to_county_df.drop_duplicates(subset=['city'])
print(cities_to_county_df.shape[0])
cities_to_county_df.head()

"""### Cost of Living Data Extraction & Cleaning

*   We added cost of living dataset to see if the cost of living has any relationship in helping predict the housing price
*   We also extracted the cost of living for California
"""

# Read datasets
cost_of_living_us_df = pd.read_csv('cost-of-living-us.csv', on_bad_lines='skip', low_memory=False)
cost_of_living_us_df.head()

# Number of rows in cost of living dataset
cost_of_living_us_df.shape[0]

# Drop the na and set isMetro to 0/1
cost_of_living_us_df = cost_of_living_us_df.dropna()
cost_of_living_us_df['isMetro'] = cost_of_living_us_df['isMetro'].apply(lambda x: 1 if x else 0)
cost_of_living_us_df.head()

# Set the cost of living for California
cost_of_living_ca_df = cost_of_living_us_df[cost_of_living_us_df['state']=='CA']

"""### Bay area cities extraction & cleaning

*   We added a bay area dataset to help retrieve all the cities in the bay area
"""

# Read and get list of all Bay Area Cities
bay_area_cities = pd.read_csv('bay-area.csv')
bay_area_cities = bay_area_cities.rename(columns={'country_code': 'Country', 'city_name': 'City' })
bay_area_cities_df = bay_area_cities[['Country', 'City']]
bay_area_cities_df.head(5)

"""### US Pollution Data extraction & cleaning

*   We extracted US Pollution dataset to see if the pollution level affects the housing market
"""

# Read air pollution data
air_pollution_df = pd.read_csv('air-pollution.csv')
air_pollution_df.head()

# Filter air pollution data to USA only into us_pollution_df
us_pollution_df = air_pollution_df[air_pollution_df['Country']=='United States of America'].copy()
us_pollution_df['Country'] = 'US'
# Only get the Country City and AQI Value column
us_pollution_df = us_pollution_df[['Country', 'City', 'AQI Value']]
us_pollution_df.head(5)

# Group by Bay area cities and AQI and another by non Bay Area Cities
aqi_bay_area_cities_df = us_pollution_df[us_pollution_df['City'].isin(bay_area_cities_df['City'])]
aqi_non_bay_area_cities_df = us_pollution_df[~us_pollution_df['City'].isin(bay_area_cities_df['City'])]
aqi_bay_area_cities_df = aqi_bay_area_cities_df[['City', 'AQI Value']]
aqi_non_bay_area_cities_df = aqi_non_bay_area_cities_df[['City', 'AQI Value']]
aqi_bay_area_cities_df

"""### US Tech Company Data extracting & cleaning

*   We extracted some big US tech company to predict if that impacts the housing market
"""

# Read USA tech companies
us_tech_companies_df = pd.read_csv('us-tech-companies.csv')
us_tech_companies_df.head(5)

"""### Temperatures extraction & cleaning

*   We extract some of the temperature information to predict if that impacts the housing market
"""

# Read major US city average monthly temp for past few decades
us_city_temp_avg_df = pd.read_csv('us-city-avg-temp.csv')
us_city_temp_avg_df.head(5)

# Filter for Bay Area Cities and other cities
bay_area_cities_avg_temp_df = us_city_temp_avg_df[['time', 'san_francisco', 'san_antonio', 'sacramento', 'richmond']]
other_cities_avg_temp_df = us_city_temp_avg_df.drop(['san_francisco', 'san_antonio', 'sacramento', 'richmond'], axis=1)
bay_area_cities_avg_temp_df.head(5)

"""### Zoning Data extraction & cleaning

*   Following the adoption of the Regional Housing Need Assessment (RHNA), each jurisdiction is required to update its housing element to identify sites adequate to accommodate the number of units allocated to them
"""

# Read in the zone dataset
zoning = pd.read_csv('zoning.csv')
zoning=zoning.rename(columns={'jurisdict':'jurisdiction',
                       'locacres':'acreage',
                       'currunits':'units',
                       'allowden':'allowed_unit_density',
                       'desafford':'affordability',
                       'infcapcty': 'infrastructure_capacity',
                       'sitetype': 'site_status'})
zoning = zoning[['county', 'jurisdiction', 'zoning', 'acreage', 'units', 'allowed_unit_density', 'affordability', 'infrastructure_capacity', 'site_status']]
zoning = zoning.dropna(subset=['zoning'], axis=0)
zoning.head()

# Convert the zone code to type
zone_types = {
    'R': 'Residential',
    'C': 'Commercial',
    'P': 'Planned Development',
    'A': 'Agricultural',
    'T': 'Mixed-Use',
    'D': 'Downtown',
    'M': 'Industrial',
    'U': 'Urban',
    'N': 'Neighborhood Commercial'
}

def zone_code_to_type(zone):
    if pd.isna(zone):
        return 'Unknown'
    if any(c for c in zone if c.islower()):
        return zone
    c = zone[0]
    return zone_types.get(c, 'Other')

zoning['zoning_type'] = zoning['zoning'].apply(lambda x: zone_code_to_type(x))
zoning.head()

"""### Proposition 21 voter data (2020), SF precinct extraction & cleaning
*   Final: Yes 40.1%, No 59.9%

*   Pro: This would let cities pass limits on rent increases to protect California families who are one rent hike away from being driven out of their neighborhoods by corporate landlords. This will stop more homelessness and gentrification.

*   Anti: It would make it less profitable for builders to construct more housing, affordable or not, at a time when California has a massive housing shortage. It would also decrease revenue for city and state governments, already cash-strapped by the pandemic.
"""

# Read in the voter dataset
prop21 = pd.read_csv('prop-21.csv')

# Rename the Unnamed 4 column to No%
prop21 = prop21.rename(columns={'Unnamed: 4': 'No%'})
prop21 = prop21[prop21['Precinct']!='Election Day']
prop21 = prop21[prop21['Precinct']!='Vote by Mail'].reset_index().drop(columns='index')

# Update the Precinct column
for i in range(1, len(prop21), 2):
    precinct = prop21.iloc[i-1]['Precinct']
    prop21.at[i, 'Precinct'] = precinct
prop21 = prop21.dropna(axis=0)

# Apply function to fix Precinct, Yes% and No% columns
prop21['Precinct'] = prop21['Precinct'].apply(lambda x: x.replace('PCT ', ''))
prop21['Yes%'] = prop21['Yes%'].apply(lambda x: float(x.strip('%'))/100)
prop21['No%'] = prop21['No%'].apply(lambda x: float(x.strip('%'))/100)

# Split based on /
to_split = prop21[prop21['Precinct'].str.contains('/', na=False)]
inds_to_split = to_split.index
df_split = to_split['Precinct'].str.split('/', expand=True)

rows = []
for index, row in to_split.iterrows():
    for precinct in df_split.loc[index]:
        new_row = row.copy()
        new_row['Precinct'] = precinct
        rows.append(new_row)
new_df = pd.DataFrame(rows)

prop21 = prop21.drop(inds_to_split, axis=0)
prop21 = pd.concat([prop21, new_df])
prop21.head()

# Read in the San Francisco precinct data
sf_precincts = pd.read_csv('sf-precincts.csv')
sf_precincts = sf_precincts[['prec_2012', 'the_geom', 'neighrep']]
sf_precincts= sf_precincts.dropna(axis=0)
sf_precincts['prec_2012'] = sf_precincts['prec_2012'].astype(int).astype(str)
sf_precincts.head()

"""### California Housing extraction & cleaning

*   We extracted the California dataset to predict if any of the features on this dataset affect the housing market
"""

# Read in the California dataset and the number of rows
ca_housing = pd.read_csv('ca-housing.csv')
ca_housing.head()
len(ca_housing)

# Convert the proximity map to numbers
proximity_map = {
    'ISLAND':0,
    'NEAR OCEAN':1,
    'NEAR BAY': 2,
    '<1H OCEAN':3,
    'INLAND':4
}

ca_housing['ocean_proximity'] = ca_housing['ocean_proximity'].apply(lambda x: proximity_map[x])
ca_housing.head()

# Longitude and Latitude of each cites
cities_lat_lon = pd.read_csv('cal_cities_lat_long.csv')
cities_lat_lon.head()

from scipy.spatial import cKDTree

city_coords = cities_lat_lon[['Latitude', 'Longitude']].values
tree = cKDTree(city_coords)

# Finding the nearest city based on longitude and latitude
def find_nearest_city(row):
    house_coords = np.array([row['latitude'], row['longitude']])
    dist, ind = tree.query(house_coords, k=1)
    return cities_lat_lon.iloc[ind]['Name']

ca_housing['city'] = ca_housing.apply(find_nearest_city, axis=1)

ca_housing = ca_housing.merge(cities_to_county_df, left_on='city', right_on='city')
ca_housing.head()

"""# EDA (Exploratory Data Analysis)

*   Perform EDA on each of the dataset we extracted and cleaned

### US real estate EDA
"""

# Find average cost of a home by state using realtor_df
avg_state_price_df = realtor_df.groupby('state')['price'].mean().reset_index()
avg_state_price_df = avg_state_price_df.rename(columns={'price': 'avg_price'})
avg_state_price_df = avg_state_price_df.sort_values(by='avg_price', ascending=False)
avg_state_price_df.head(5)

"""*   The top 5 states with the highest average housing price is Hawaii, California, New York, DC, and Utah

### State to Abbreviated State

*   Dataset to help convert state to is abbreviation and vice versa
"""

# Preprocess State names to their abbreviations for plotly express later
us_state_to_abbv = { #credit to https://gist.github.com/rogerallen/1583593 for dict
    "Alabama": "AL",
    "Alaska": "AK",
    "Arizona": "AZ",
    "Arkansas": "AR",
    "California": "CA",
    "Colorado": "CO",
    "Connecticut": "CT",
    "Delaware": "DE",
    "Florida": "FL",
    "Georgia": "GA",
    "Hawaii": "HI",
    "Idaho": "ID",
    "Illinois": "IL",
    "Indiana": "IN",
    "Iowa": "IA",
    "Kansas": "KS",
    "Kentucky": "KY",
    "Louisiana": "LA",
    "Maine": "ME",
    "Maryland": "MD",
    "Massachusetts": "MA",
    "Michigan": "MI",
    "Minnesota": "MN",
    "Mississippi": "MS",
    "Missouri": "MO",
    "Montana": "MT",
    "Nebraska": "NE",
    "Nevada": "NV",
    "New Hampshire": "NH",
    "New Jersey": "NJ",
    "New Mexico": "NM",
    "New York": "NY",
    "North Carolina": "NC",
    "North Dakota": "ND",
    "Ohio": "OH",
    "Oklahoma": "OK",
    "Oregon": "OR",
    "Pennsylvania": "PA",
    "Rhode Island": "RI",
    "South Carolina": "SC",
    "South Dakota": "SD",
    "Tennessee": "TN",
    "Texas": "TX",
    "Utah": "UT",
    "Vermont": "VT",
    "Virginia": "VA",
    "Washington": "WA",
    "West Virginia": "WV",
    "Wisconsin": "WI",
    "Wyoming": "WY",
    "District of Columbia": "DC",
    "American Samoa": "AS",
    "Guam": "GU",
    "Northern Mariana Islands": "MP",
    "Puerto Rico": "PR",
    "United States Minor Outlying Islands": "UM",
    "Virgin Islands, U.S.": "VI",
}
avg_state_price_df['state_abbv'] = avg_state_price_df['state'].map(us_state_to_abbv)
avg_state_price_df.head(5)

# Convert state to its abbreviation
realtor_df['state'] = realtor_df['state'].map(us_state_to_abbv)
realtor_df.head()

"""### Map of Average Cost by States"""

# Generate a map visualization of Average Cost of a home by State
fig = px.choropleth(avg_state_price_df,
  locations='state_abbv',
  locationmode="USA-states",
  color='avg_price',
  color_continuous_scale="purples",
  scope="usa",
  labels={'avg_price': 'Average Housing Price'}
)

fig.update_layout(
    title_text='Average Cost of Housing by State',
    geo=dict(
        bgcolor='black',
    )
)

fig.show()

"""According to this figure, the most expensive houses on average by State come from Hawaii, California, and New York. But lets dive deeper into the state of California, especially the Bay Area Region. This gives us a general understanding of housing prices, but lets look deeper.

### AQI and real estate data EDA (Combined datasets)
"""

aqi_bay_area_cities_df

# Drop nulls in realtor_df and combine with us_air_pollution_df by city
us_pollution_df.rename(columns={'City': 'city'}, inplace=True)
realtor_df.dropna(subset=['city'], inplace=True)
realtor_df.reset_index(drop=True, inplace=True)
realtor_df.head()
realtor_aqi_df = realtor_df.merge(us_pollution_df, on='city', how='inner')
realtor_aqi_df['AQI Value'].fillna(realtor_aqi_df.groupby('city')['AQI Value'].transform('mean'), inplace=True)
realtor_aqi_df['house_size'].fillna(realtor_aqi_df['house_size'].mean(), inplace=True)
realtor_aqi_df.head(9)

# Number of rows in the Realtor AQI dataset
realtor_aqi_df.shape[0]

# Plot AQI Values for bay area cities vs Non-bay area cities
fig, axes = plt.subplots(1, 2, figsize=(10, 5))
aqi_bay_area_cities_df['AQI Value'].plot(
    kind='hist', bins=20, range=(0, 300), ax=axes[0], color='blue', title='Bay Area AQI Values'
)
axes[0].spines[['top', 'right']].set_visible(False)

aqi_non_bay_area_cities_df['AQI Value'].plot(
    kind='hist', bins=20, range=(0, 300), ax=axes[1], color='red', title='Non-Bay Area AQI Values'
)
axes[1].spines[['top', 'right']].set_visible(False)

for ax in axes:
    ax.set_xlabel('AQI Value')
    ax.set_ylabel('Frequency')

plt.tight_layout()
plt.show()

"""*   The Comparison of Bay area AQI vs non Bay Area AQI shows us that AQI values are mostly similar with slightly higher AQI for non Bay Area Cities."""

# Find the California housing under 5 million dollars
ca_under_5mil = california_df[california_df['price']<= 5000000]
rest_us = realtor_df[(realtor_df['state'] != 'California') & (realtor_df['zip_code'].isin(combined_zipcodes)==False)]
us_under_5mil = rest_us[rest_us['price']<= 5000000]

# Plot the Distribution of Housing Prices in the Bay Area vs US
fig, axes = plt.subplots(1, 2, figsize=(14, 6))
sns.histplot(ca_under_5mil['price'], kde=True, bins=50, ax=axes[0])
axes[0].set_title('Distribution of Housing Prices in the Bay Area')
axes[0].set_xlabel('Price')
axes[0].set_ylabel('Frequency')

sns.histplot(us_under_5mil['price'], kde=True, bins=50, ax=axes[1])
axes[1].set_title('Distribution of Housing Prices in the US')
axes[1].set_xlabel('Price')
axes[1].set_ylabel('Frequency')

plt.show()

"""*   Distribution of housing costs in the Bay Area vs. the rest of the US.

### Cost of Living EDA

*   Explore the cost of living dataset and each of the features in this dataset
"""

# For each state, average the food cost
states = cost_of_living_us_df['state'].unique()
avg_food_cost_df = cost_of_living_us_df.groupby('state')['food_cost'].mean()
avg_food_cost_df = avg_food_cost_df.sort_index()

# Plot average cost of food of each state
plt.figure(figsize=(15,8))
plt.bar(avg_food_cost_df.index, avg_food_cost_df.values.astype(float))
plt.title("Average Cost of Food By State")
plt.xlabel("State")
plt.ylabel("Average Cost")
plt.show()

avg_food_cost_df = avg_food_cost_df.sort_values(ascending=False)
avg_food_cost_df.head(10)

"""*   California is rank 9 in terms of average food cost with Hawaii being the highest"""

# For each state, average the transportation cost
states = cost_of_living_us_df['state'].unique()
avg_transportation_cost = cost_of_living_us_df.groupby('state')['transportation_cost'].mean()
avg_transportation_cost = avg_transportation_cost.sort_index()

# Plot average cost of transportation of each state
plt.figure(figsize=(15,8))
plt.bar(avg_transportation_cost.index, avg_transportation_cost.values.astype(float))
plt.title("Average Cost of Transportation By State")
plt.xlabel("State")
plt.ylabel("Average Cost")
plt.show()

avg_transportation_cost_sort = avg_transportation_cost.sort_values(ascending=False)
avg_transportation_cost_sort.head(10)

"""*   California is ranked 3rd in terms of transportation with Utah being the highest average transportation cost"""

# For each state, average the housing cost
states = cost_of_living_us_df['state'].unique()
avg_housing_cost = cost_of_living_us_df.groupby('state')['housing_cost'].mean()
avg_housing_cost = avg_housing_cost.sort_index()

plt.figure(figsize=(15,8))
plt.bar(avg_housing_cost.index, avg_housing_cost.values.astype(float))
plt.title("Average Cost of Housing By State")
plt.xlabel("State")
plt.ylabel("Average Cost")
plt.show()

avg_housing_cost_sort = avg_housing_cost.sort_values(ascending=False)
avg_housing_cost_sort.head(10)

"""*   California is ranked 4th in terms of housing cost with Hawaii having the highest"""

# For each state, average the healthcare cost
states = cost_of_living_us_df['state'].unique()
avg_healthcare_cost = cost_of_living_us_df.groupby('state')['healthcare_cost'].mean()
avg_healthcare_cost = avg_healthcare_cost.sort_index()

plt.figure(figsize=(15,8))
plt.bar(avg_healthcare_cost.index, avg_healthcare_cost.values.astype(float))
plt.title("Average Cost of Healthcare By State")
plt.xlabel("State")
plt.ylabel("Average Cost")
plt.show()

avg_healthcare_cost_sort = avg_healthcare_cost.sort_values(ascending=False)
avg_healthcare_cost_sort.head(10)

"""*   California is not on the top list in terms of average healthcare cost"""

# For each state, average the childcare cost
states = cost_of_living_us_df['state'].unique()
avg_childcare_cost = cost_of_living_us_df.groupby('state')['childcare_cost'].mean()
avg_childcare_cost = avg_childcare_cost.sort_index()

plt.figure(figsize=(15,8))
plt.bar(avg_childcare_cost.index, avg_childcare_cost.values.astype(float))
plt.title("Average Cost of Child Care By State")
plt.xlabel("State")
plt.ylabel("Average Cost")
plt.show()

avg_childcare_cost_sort = avg_childcare_cost.sort_values(ascending=False)
avg_childcare_cost_sort.head(10)

"""*   California is not on the top average child care cost"""

# For each state, average the necessities cost
states = cost_of_living_us_df['state'].unique()
avg_other_necessities_cost = cost_of_living_us_df.groupby('state')['other_necessities_cost'].mean()
avg_other_necessities_cost = avg_other_necessities_cost.sort_index()

plt.figure(figsize=(15,8))
plt.bar(avg_other_necessities_cost.index, avg_other_necessities_cost.values.astype(float))
plt.title("Average Cost of Other Necessities By State")
plt.xlabel("State")
plt.ylabel("Average Cost")
plt.show()

avg_other_necessities_cost_sort = avg_other_necessities_cost.sort_values(ascending=False)
avg_other_necessities_cost_sort.head(10)

"""*   California is ranked the 4th in terms of average necessities cost and Hawaii has the highest"""

# For each state, average the total cost
states = cost_of_living_us_df['state'].unique()
avg_total_cost = cost_of_living_us_df.groupby('state')['total_cost'].mean()
avg_total_cost = avg_total_cost.sort_index()

plt.figure(figsize=(15,8))
plt.bar(avg_other_necessities_cost.index, avg_other_necessities_cost.values.astype(float))
plt.title("Average Cost of Total Cost By State")
plt.xlabel("State")
plt.ylabel("Average Cost")
plt.show()

avg_total_cost_sort = avg_total_cost.sort_values(ascending=False)
avg_total_cost_sort.head(10)

"""*   California is ranked the 4th in terms of total cost and DC has the highest"""

# For each state, average the median family income cost
states = cost_of_living_us_df['state'].unique()
avg_median_family_income = cost_of_living_us_df.groupby('state')['median_family_income'].mean()
avg_median_family_income = avg_median_family_income.sort_index()

plt.figure(figsize=(15,8))
plt.bar(avg_median_family_income.index, avg_median_family_income.values.astype(float))
plt.title("Average Median Income By State")
plt.xlabel("State")
plt.ylabel("Average Cost")
plt.show()

avg_median_family_income_sort = avg_median_family_income.sort_values(ascending=False)
avg_median_family_income_sort.head(10)

"""*   California is ranked the 9th in terms of average median family income where DC is the highest"""

# Create a heat map between the different costs for a few states in comparison with CA
cost_df = cost_of_living_us_df[['housing_cost', 'food_cost', 'transportation_cost', 'healthcare_cost', 'other_necessities_cost', 'childcare_cost', 'total_cost', 'median_family_income']]

states = cost_of_living_us_df['state'].unique()
state_corrs = {}

for state in states:
  state_info = cost_df[cost_of_living_us_df['state'] == state]
  state_corrs[state] = state_info.corr()

for state in ['CA', 'HI', 'NY', 'TX']:
  plt.figure(figsize=(6,6))
  sns.heatmap(state_corrs[state], annot=True, cmap='RdBu', center=0)
  plt.title(f"Correlation Heatmap in {state}")
  plt.show()

"""*   According the to heatmap, housing cost plays a big role in the total cost in CA. Similar behavior can be seem in other states as well especially Hawaii and New York.

* Other necessity cost and median family  income also has a high correlation with housing cost

### Salaries in San Francisco EDA
"""

salaries_sf_df.head()
  # Find the top paying jobs
  top_paying_jobs = salaries_sf_df.groupby('JobTitle')['TotalPay'].mean().sort_values(ascending=False)
  top_paying_jobs.head(10)

"""*   Top paying salary in San Francisco based on this dataset are General Manager in MTA, Chief Investment Officer, and Chief of Police

### Median Family Income vs. Housing Price (Combined datasets)
"""

# Add county column to the realtor dataset to merge with cost of living dataset
realtor_county = realtor_df.merge(cities_to_county_df, on='city', how='inner')
realtor_county = realtor_county.drop_duplicates()
realtor_county = realtor_county.rename(columns={'county_name': 'county'})
realtor_county.head()

# Merge realtor and cost of living dataset
realtor_county['county'] = realtor_county['county'].str.replace(' County', '', regex=False).str.lower()
cost_of_living_us_df['county'] = cost_of_living_us_df['county'].str.replace(' County', '', regex=False).str.lower()

# Filter the realtor price by cities with only the following columns: price, city, state, and county
realtor_price_by_cites_df = realtor_county[['price', 'city','state', 'county']]

# Filter the cost of living by cities with only the following columns: state, median family income, and county
cost_of_living_per_cities_df = cost_of_living_us_df[['state', 'median_family_income', 'county']]

print(realtor_price_by_cites_df.head())
print(cost_of_living_per_cities_df.head())

# Finish cost of living merge dataset
cost_of_living_merge = realtor_price_by_cites_df.merge(cost_of_living_per_cities_df, on=['county', 'state'], how='inner')
cost_of_living_merge = cost_of_living_merge.drop_duplicates().dropna().reset_index(drop=True)
# print(cost_of_living_merge.shape[0])
cost_of_living_merge.head()

# Plot the Price of housing with median family income
plt.scatter(cost_of_living_merge['median_family_income'], cost_of_living_merge['price'], s=2)
plt.title('Price of Housing vs Median Family Income')
plt.xlabel('Median Family Income')
plt.ylabel('Price of Housing')
plt.show()

"""*   Used a scatter plot, there seems to have some outlier in the dataset and not a strong correlation

### Proposition 21 voter data and SF precinct EDA (Combined datasets)
"""

# Merge prop21 and sf_precinct datasets
merged_df = sf_precincts.merge(prop21, left_on='prec_2012', right_on='Precinct')
merged_df['geometry'] = merged_df['the_geom'].apply(lambda x: loads(x))
merged_df.head(3)

# Coordinates for San Francisco
sf_lat, sf_lon = 37.7749, -122.4194
m = folium.Map(location=[sf_lat, sf_lon], zoom_start=12)

colormap = linear.YlOrRd_09.scale(min(merged_df['Yes%']), max(merged_df['Yes%']))
heat_data = []
for idx, row in merged_df.iterrows():
    yes_per = row['Yes%']
    color = colormap(yes_per)
    if isinstance(row['geometry'], MultiPolygon):
          for geom in row['geometry'].geoms:
            folium.GeoJson(
                geom.__geo_interface__,
                style_function=lambda x, color=color: {
                    'fillColor': color,
                    'fillOpacity': 0.5,
                    'color': 'black',
                    'weight': 1
                }
            ).add_to(m)
            for x, y in geom.exterior.coords:
              heat_data.append([x, y, yes_per])

HeatMap(heat_data, min_opacity=0.2, max_val=100, gradient={0.0: 'yellow', 0.5: 'orange', 1.0: 'red'}).add_to(m)
colormap.add_to(m)
m

"""*   Heatmap of votes on Proposition 21 in 2020 aggregated by precinct. The darker the red, the more pro-rent control and against building more housing the votes were.

### California Housing Data EDA
Note: this data is from more than 20 years ago, so the distribution may be very different.
"""

ca_housing['median_house_value_adjusted'] = ca_housing['median_house_value'].apply(lambda x: x*1.71)

# Plot the distribution of median housing values
plt.figure(figsize=(6, 4))
plt.hist(ca_housing['median_house_value_adjusted'], bins=30, edgecolor='black')
plt.title('Distribution of Median House Values (Adjusted)')
plt.xlabel('Median House Value')
plt.ylabel('Frequency')
plt.show()

"""

*   There seem to be a general downward trend as the median housing price increase the frequency decrease except after the median housing price hits above 800000

"""

# Plot the median house value
plt.figure(figsize=[8,6])
plt.ylim(32,42)
plt.xlim(-125,-114)
plt.ylabel("Latitude")
plt.xlabel("Longitude")
plt.title("Median House Value")
plt.scatter(x = ca_housing["longitude"],
            y = ca_housing["latitude"],
            c = ca_housing["median_house_value_adjusted"],
            cmap="coolwarm",
            s=1)
plt.colorbar()
plt.show()

"""

*   The trend is closer to the Bay the higher the housing price which match our expectation

"""

# Created a heat map based on the median housing value
m = folium.Map(location=[sf_lat, sf_lon], zoom_start=9)

max_value = ca_housing['median_house_value_adjusted'].max()
heat_data = [
    [row['latitude'], row['longitude'], row['median_house_value_adjusted'] / max_value]
    for idx, row in ca_housing.iterrows()
]

HeatMap(heat_data, radius=10, blur=6, max_zoom=1).add_to(m)
colormap = cm.LinearColormap(
    colors=['blue', 'yellow', 'red'],
    vmin=ca_housing['median_house_value_adjusted'].min(),
    vmax=ca_housing['median_house_value_adjusted'].max(),
    caption='Median House Value'
)
colormap.add_to(m)
m

"""*   Similar trend, closer to Bay Area the higher the median housing price

### Tech companies EDA

### Zoning EDA
"""

# Group zoning by acreage sum
zoning.groupby('zoning_type')['acreage'].sum().sort_values(ascending=False).head()

residential = zoning[zoning['zoning_type']=='Residential']

# Get number of single family residential zoning areas
residential['single_family'] = residential['zoning'].apply(lambda x: 1 if 'R-1' in x else 0)
residential.groupby('single_family')['zoning'].count()

# Get number of residential areas that have affordable housing programs
residential['affordability'] = residential['affordability'].fillna(0)
residential['affordability'] = residential['affordability'].apply(lambda x: 1 if x != 0 else x)
residential.groupby('affordability')['zoning'].count()

"""### Cost of living & real estate/housing EDA (Combined datasets)"""

cost_of_living_ca_df = cost_of_living_ca_df.drop(columns=['case_id', 'areaname'])
cost_of_living_ca_df['county'] = cost_of_living_ca_df['county'].apply(lambda x: x.replace(' County', ''))
cost_of_living_ca_df.head()

# Cost of living â€“ aggregate by county
col_ca_by_county_df = cost_of_living_ca_df.drop(columns=['family_member_count','state']).groupby('county').mean()
col_ca_by_county_df=col_ca_by_county_df.reset_index()
col_ca_by_county_df.head()

# Merging cities_to_county_df to add county column to real estate data, since cost of living df only has data aggregated by county
ca_real_estate = california_df.merge(cities_to_county_df, left_on='city', right_on='city')
ca_real_estate.head()

# Merge CA real estate with cost of living in that county
col_housing_merged = ca_real_estate.merge(col_ca_by_county_df, left_on='county_name', right_on='county')
col_housing_merged.head()

# CA housing data aggregated by city
ca_housing_by_city = ca_housing.drop(columns='county_name')
ca_housing_by_city = ca_housing_by_city.groupby('city').agg({
    'housing_median_age': 'mean',
    'total_rooms':'sum',
    'total_bedrooms':'sum',
    'population':'sum',
    'households':'sum',
    'median_income':'mean',
    'median_house_value':'mean',
    'ocean_proximity':'mean'
})
ca_housing_by_city.reset_index(inplace=True)
ca_housing_by_city.head()

# Continue merging
col_housing_merged = col_housing_merged.merge(ca_housing_by_city, on='city')
col_housing_merged = col_housing_merged.merge(us_pollution_df, on='city')
col_housing_merged=col_housing_merged.drop(columns=['county_name','median_income', 'street','other_necessities_cost', 'total_cost'])
col_housing_merged.head()

# Renaming columns for clarity
col_housing_merged = col_housing_merged.rename({
    'housing_cost':'county_avg_housing_cost',
    'food_cost': "county_avg_food_cost",
    'transportation_cost': "county_avg_transport_cost",
    'healthcare_cost':'county_avg_heathcare_cost',
    'childcare_cost':'county_avg_childcare_cost',
    'taxes':'county_avg_taxes',
    'median_family_income': 'county_avg_family_income',
    'housing_median_age': 'city_avg_housing_age',
    'total_rooms':'total_rooms_in_city',
    'total_bedrooms':'total_bedrooms_in_city',
    'population':'city_population',
    'households': 'num_households_in_city',
    'median_house_value': 'city_avg_house_value',
    'ocean_proximity': 'city_ocean_proximity'
  }, axis=1)

# Number of rows on the col_housing_merged dataset
col_housing_merged.shape[0]

# col_housing_merged.describe()
# Split the price into catorgories
percentiles = [0.2, 0.4, 0.6, 0.8, 1.0]
percentile_values = col_housing_merged['price'].quantile(percentiles)
print(percentile_values)

home_price_classes = {
    5: [1250001, 515000000], # luxury
    4: [799001, 1250000], # high end
    3:[599001,799000], # mid tier
    2:[415001, 599000], # low end
    1:[0, 415000] # very affordable
}

# Get the price categories based on the price of the house
def get_price_class(price):
  for category, (low, high) in home_price_classes.items():
    if low <= price <= high:
      return category

col_housing_merged['price_class'] = col_housing_merged['price'].apply(get_price_class)
col_housing_merged.head()

"""# Modeling Training & Prediction

### Package Import & Helper Function
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score
from sklearn.preprocessing import OneHotEncoder

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.svm import SVC

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""

*   Tried out linear regression between median family income and housing price

* Based on the result, this is a poor fit of the model




"""

# Linear regression on median family income vs. home price
X = cost_of_living_merge[['median_family_income']]
y = cost_of_living_merge['price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mean_sq_error = mean_squared_error(y_test, y_pred)

print("Mean Squared Error:", mean_sq_error)

"""Using linear regression on housing price resulted in very high MSE so we switch our model to classification.

### Modeling on col_housing_merged
This dataset is 121,016 rows: real estate, cost of living, and CA data merged.
"""

col_housing_merged = col_housing_merged.dropna(axis=0)
X = col_housing_merged[['bed', 'bath', 'acre_lot', 'zip_code', 'house_size',
                        'isMetro', 'county_avg_housing_cost', 'county_avg_food_cost',
                        'county_avg_transport_cost', 'county_avg_heathcare_cost',
                        'county_avg_childcare_cost', 'county_avg_taxes',
                        'county_avg_family_income', 'city_avg_housing_age',
                        'total_rooms_in_city', 'total_bedrooms_in_city', 'city_population',
                        'num_households_in_city', 'city_avg_house_value', 'city_ocean_proximity']]
y = col_housing_merged['price_class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Finding optimal n_components for PCA"""

pca = PCA()
pca.fit(X)
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance_ratio)

plt.figure(figsize=(8,5))
plt.plot(range(1, len(cumulative_variance)+1), cumulative_variance, marker='o', linestyle='-')
plt.title('Cumulative Explained Variance vs. PCA Components')
plt.xlabel('Number of principal components')
plt.ylabel('Cumulative Explained Variance')
plt.show()

"""PCA and Logistic Regression (Accuracy: 58.6%)


"""

# PCA and LogReg

scaler = StandardScaler()
pca = PCA(n_components=5)
lr = LogisticRegression(multi_class='multinomial',
                        solver='lbfgs',
                        max_iter=1000)

pipe = Pipeline(steps=[('Scale',scaler), ('PCA',pca), ('LogReg',lr)])
pipe.fit(X_train,y_train)
# y_pred = pipe.predict(X_test)

print('PCA and Logistic Regression on col_housing_merged:')
log_acc = pipe.score(X_test, y_test)
print("Accuracy:", log_acc)

"""PCA and Random Forest Classifier (Accuracy: 70.5%)"""

# PCA and Random Forest

scaler = StandardScaler()
pca = PCA(n_components=5)
rf = RandomForestClassifier(class_weight='balanced',
                            criterion='entropy',
                            n_estimators=150,
                            max_depth=30,
                            random_state=42)

X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train_pca, y_train)

y_pred = rf.predict(X_test_pca)

print('Random Forest Classifier with PCA on col_housing_merged')
rf_acc = rf.score(X_test_pca, y_test)
print("Accuracy:", rf_acc)
rf_confusion = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(rf_confusion)

print(classification_report(y_test, y_pred))

pca.explained_variance_ratio_

"""Random Forest Classifier only (Accuracy: 77.1%)"""

# Random Forest WITHOUT PCA

rfc = RandomForestClassifier(n_estimators=150,
                             class_weight='balanced',
                             criterion='entropy',
                             max_depth=30,
                             random_state=42)
rfc.fit(X_train, y_train)
y_pred = rfc.predict(X_test)

print('Random Forest Classifier on col_housing_merged')
rf_acc = rfc.score(X_test, y_test)
print("Accuracy:", rf_acc)
rf_confusion = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(rf_confusion)

print(classification_report(y_test, y_pred))

"""Feature Importances from Random Forest Classification"""

feature_importances = rfc.feature_importances_
features = X.columns

ind = np.argsort(feature_importances)[::-1]
feature_names = [features[i] for i in ind]
importances = feature_importances[ind]

plt.figure(figsize=(10, 6))
plt.bar(feature_names, importances)

plt.title('Feature Importances')
plt.ylabel('Importance')
plt.xlabel('Features')
plt.xticks(rotation=45, ha='right')

plt.tight_layout()
plt.show()

"""# Challenges and Obstacles Faced

*   Each dataset has a unique set of columns and keys, making it challenging to identify a common key for merging and combining the two datasets.
*   Some datasets are large, and merging too many features can sometimes lead to out-of-memory issues.
*   Searching for certain datasets was very difficult since some were behind a paywall or from a private company.

# Next Steps / Future Direction

*   So far we only focused on the Bay Area compared to the US, in the future we could compare Bay Area cities to other major cities in other countries

*   We could also explore other main cities in the US beside the Bay area such as Hawaii area, DC area, or New York area

*   We could convert currency to balance out housing prices in USD for those other countries

*    We could also run additional model to try to get better results
"""